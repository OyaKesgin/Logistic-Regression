2.1.2 Newton's method

def inv(m):
    """inverse matrix"""  
    a, b = m.shape
    if a != b:
        raise ValueError("Square matrices are invertible.")
    i = np.eye(a, a)
    
    return np.linalg.lstsq(m, i)[0]
    
    def getLoss_n(y, tx, w, lam):
    """return the loss and updated w"""
    n = y.shape[0]
    p = sigmoid(tx.dot(w))
    
    loss = (-1 / n) * np.sum((y * np.log(p) + (1 - y) * np.log(1 - p))) + (lam / 2) * np.sum(w * w)
    grad = (-1 / n) * np.dot(tx.T, (y - p))+ lam / n * w
    Sw = np.diag(p*(1-p)) 
    TXT=tx.T.dot(tx)   
    hessian = Sw * TXT/ y.shape[0] 
    
    w = w - np.dot(inv(hessian), grad)
    
    return loss, w


def logistic_regression_newton_method_demo(y, x):
    """run logistic regression with newton method"""      
    startTime = time.time()
    
    lam = 0
    iteration = 8  
    threshold = 1e-8
    losses = []

    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))
    w[0,1:] = w[0,1:] - lam * w[0,1:]
    
    for iter in range(iteration):
        loss, w = getLoss_n(y, tx, w, lam)
        if iter % 1 == 0:
            print("Current iteration={i}, the loss={l}".format(i=iter, l=loss))
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    print('Time(s):',time.time() - startTime) 
    
    plt.plot(losses)
    plt.ylabel('Loss')
    plt.xlabel('Iteration')
    
    return w

w = logistic_regression_newton_method_demo(y2, x2)

accuracy_n,pred_n = getAccuracy(x2, y2, w)
act_n = y2
accuracy_tst_n, pred_tst_n = getAccuracy(x2_tst, y2_tst, w)
act_tst_n = y2_tst

print ('Training Accuracy (Binary, Newton):', accuracy_n)
print ('Test Accuracy (Binary, Newton):',accuracy_tst_n)

cm_n = metrics.confusion_matrix(act_n, pred_n)
print (cm_n)

sns.heatmap(cm_n, annot = True, fmt="d", linewidths=.5, square = True,cmap='Blues')
plt.ylabel('Actual Number')
plt.xlabel('Predicted Number')
score = round(accuracy_n,3)
plt.title("Newton's method (Training Set)\nAccutacy: {0}".format(score),size=15)
plt.figure(figsize=(5,5))

print(classification_report(act_n, pred_n))

cm_tst_n = metrics.confusion_matrix(act_tst_n, pred_tst_n)
print (cm_tst_n)

sns.heatmap(cm_tst_n, annot = True, fmt="d", linewidths=.5, square = True,cmap='Blues')
plt.ylabel('Actual Number')
plt.xlabel('Predicted Number')
score = round(accuracy_tst_n,3)
plt.title("Newton's method (Test Set)\nAccutacy: {0}".format(score),size=15)
plt.figure(figsize=(5,5))

print(classification_report(act_tst_n, pred_tst_n))

2.2 Scikit-learn

clf = LogisticRegression()

clf.fit(x2,y2)

prEdictions = clf.predict(x2_tst)
prEdictions

score = clf.score(x2_tst,y2_tst)
score

cm_sk = metrics.confusion_matrix(y2_tst,pridictions)
print (cm)

sns.heatmap(cm_sk, annot = True, fmt="d", linewidths=.5, square = True,cmap='Blues')
plt.ylabel('Actual Number')
plt.xlabel('Predicted Number')
score = round(score,3)
plt.title('Scikit-learn\nAccutacy Score:{0}'.format(score),size=15)
plt.figure(figsize=(5,5))

3 Multi-class Classification

# training set
y = labels_tr
x = images_tr
# test set
y_tst = labels_tst
x_tst = images_tst

x.shape, y.shape, x_tst.shape, y_tst.shape

3.1 Logistic Regression

def softmax(t):
    """apply softmax function on t"""    
    t -= np.max(t)
    sm = (np.exp(t).T / np.sum(np.exp(t),axis=1)).T
    
    return sm
    
def oneHot(Y):
    """convert matrix into One-hot format"""     
    tx = x.shape[0]
    OHX = scipy.sparse.csr_matrix((np.ones(tx), (Y, np.array(range(tx)))))
    OHX = np.array(OHX.todense()).T
    
    return OHX
    
 def getLoss_multi(w,x,y,lam):
    """compute loss and gradient"""
    n = y.shape[0] 
    y = oneHot(y) 
    scores = np.dot(x,w)   
    p = softmax(scores)   
    loss = (-1 / n) * np.sum((y * np.log(p) + (1 - y) * np.log(1 - p))) + (lam/2)*np.sum(w*w)
    grad = (-1 / n) * np.dot(x.T,(y - p)) + lam / n * w 
    
    return loss,grad


def softmax_regression_gradient_descent_demo(y, x):
    """run softmax regression with gradient descent"""     
    startTime = time.time()
    
    w = np.zeros([x.shape[1],len(np.unique(y))])
    
    lam = 0
    
    iteration = 300 
    learning_rate = 1e-5
    threshold = 1e-8
    losses = []

    for iter in range(iteration):
        loss, grad = getLoss_multi(w,x,y,lam)
        w = w - (learning_rate * grad) 
        w[0,1:] = w[0,1:] - lam * w[0,1:]
        if iter % 10 == 0:
            print("Current iteration={i}, the loss={l}".format(i=iter, l=loss))
        losses.append(loss)   
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break  
    print('Time(s):',time.time() - startTime)    

    plt.plot(losses)
    plt.ylabel('Loss')
    plt.xlabel('Iteration')
    
    return w

w = softmax_regression_gradient_descent_demo(y, x)

classWeightsToVisualize = 7
plt.imshow(scipy.reshape(w[:,classWeightsToVisualize],[28,28]))

def getProbsAndPreds_multi(x):
    """compute prob and pred"""    
    prob = softmax(np.dot(x,w))
    pred = np.argmax(prob,axis=1)
    
    return prob, pred
    
def getAccuracy_multi(x,y):
    """compute accuracy"""  
    prob,pred = getProbsAndPreds_multi(x)
    accuracy = sum(pred == y)/(float(len(y)))
    
    return accuracy,prob,pred
    
accuracy_multi,prob_multi,pred_multi = getAccuracy_multi(x,y)
act_multi = y
accuracy_tst_multi,prob_tst_multi,pred_tst_multi = getAccuracy_multi(x_tst,y_tst)
act_tst_multi = y_tst

print ('Training Accuracy (Multi-class, Gradient Descent):', accuracy_multi)
print ('Test Accuracy of (Multi-class, Gradient Descent):',accuracy_tst_multi)

cm_multi = metrics.confusion_matrix(act_multi, pred_multi)
print (cm_multi)

plt.figure(figsize=(11,11))
sns.heatmap(cm_multi, annot = True, fmt="d", linewidths=.5, square = True,cmap='Blues')
plt.ylabel('Actual Number')
plt.xlabel('Predicted Number')
score = round(accuracy_multi,3)
plt.title("Gradient descent (Training Set)\nAccutacy: {0}".format(score),size=15)

print(classification_report(act_multi, pred_multi))

accuracy_tst_multi,prob_tst_multi,pred_tst_multi = getAccuracy_multi(x_tst,y_tst)
act_tst_multi = y_tst

act_tst_multi.shape, pred_tst_multi.shape

cm_tst_multi = metrics.confusion_matrix(act_tst_multi, pred_tst_multi)
print (cm_tst_multi)

plt.figure(figsize=(11,11))
# sns.heatmap(cm, annot = True, fmt=".3f", linewidths=.5, square = True,camp='Blues')
sns.heatmap(cm_tst_multi, annot = True, fmt="d", linewidths=.5, square = True,cmap='Blues')
plt.ylabel('Actual Number')
plt.xlabel('Predicted Number')
score = round(accuracy_tst_multi,3)
plt.title("Gradient descent (Test Set)\nAccutacy: {0}".format(score),size=15)

print(classification_report(act_tst_multi, pred_tst_multi))

Conclusion
Accuracy

print ('Training Accuracy (Binary, Gradient Descent):', accuracy)
print ('Test Accuracy (Binary, Gradient Descent):',accuracy_tst)

print ('Training Accuracy (Binary, Newton):', accuracy_n)
print ('Test Accuracy (Binary, Newton):',accuracy_tst_n)

print ('Training Accuracy (Multi-class, Gradient Descent):', accuracy_multi)
print ('Test Accuracy of (Multi-class, Gradient Descent):',accuracy_tst_multi)
